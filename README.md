# Awesome-Medical-Healthcare-Dataset-For-LLM
A curated list of popular Datasets, Models and Papers for LLMs in Medical/Healthcare.

## Datasets

### ä¸­æ–‡

| æ•°æ®é›†åç§° | å†…å®¹æ¦‚è¿° | è·å–é“¾æ¥ | æ•°æ®å¤§å° |
|----------|--------|---------|---------|
| MedDialog | MedDialogæ•°æ®é›†ï¼ˆä¸­æ–‡ï¼‰åŒ…å«äº†åŒ»ç”Ÿå’Œæ‚£è€…ä¹‹é—´çš„å¯¹è¯ï¼ˆä¸­æ–‡ï¼‰ã€‚å®ƒæœ‰110ä¸‡ä¸ªå¯¹è¯å’Œ400ä¸‡ä¸ªè¯è¯­ã€‚æ•°æ®è¿˜åœ¨ä¸æ–­å¢é•¿ï¼Œä¼šæœ‰æ›´å¤šçš„å¯¹è¯åŠ å…¥ã€‚åŸå§‹å¯¹è¯æ¥è‡ªå¥½å¤§å¤«ç½‘ã€‚ | [ä¸‹è½½é“¾æ¥](https://huggingface.co/datasets/medical_dialog) | 3.3GB |
|Chinese medical dialogue data ä¸­æ–‡åŒ»ç–—å¯¹è¯æ•°æ®é›† | æ•°æ®ä¸­æœ‰å…­ä¸ªç§‘å®¤çš„åŒ»å­¦é—®ç­”æ•°æ®:<br/>**<Andriatria_ç”·ç§‘>** 94596ä¸ªé—®ç­”å¯¹ **<IM_å†…ç§‘>** 220606ä¸ªé—®ç­”å¯¹ **<OAGD_å¦‡äº§ç§‘>** 183751ä¸ªé—®ç­”å¯¹**<Oncology_è‚¿ç˜¤ç§‘>** 75553ä¸ªé—®ç­”å¯¹ **<Pediatric_å„¿ç§‘>** 101602ä¸ªé—®ç­”å¯¹ **<Surgical_å¤–ç§‘>** 115991ä¸ªé—®ç­”å¯¹ æ€»è®¡ 792099ä¸ªé—®ç­”å¯¹ | [ä¸‹è½½é“¾æ¥](https://github.com/Toyhom/Chinese-medical-dialogue-data) | 800k æ¡ï¼Œ330MB |
| Huatuo-26M                        | Huatuo-26M æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ä¸­åŒ»é—®ç­”æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡2600ä¸‡å¯¹é«˜è´¨é‡çš„åŒ»å­¦é—®ç­”å¯¹ï¼Œæ¶µç›–ç–¾ç—…ã€ç—‡çŠ¶ã€æ²»ç–—å’Œè¯ç‰©ä¿¡æ¯ç­‰å¹¿æ³›ä¸»é¢˜ã€‚                  | [ä¸‹è½½é“¾æ¥](https://github.com/FreedomIntelligence/Huatuo-26M)            | 4.54GB |
| huatuo_encyclopedia_qa  | è¯¥æ•°æ®é›†å…±æœ‰364,420æ¡åŒ»ç–—QAæ•°æ®ï¼Œå…¶ä¸­ä¸€äº›æ•°æ®ä»¥ä¸åŒçš„æ–¹å¼åŒ…å«å¤šä¸ªé—®é¢˜ã€‚æˆ‘ä»¬ä»çº¯æ–‡æœ¬(ä¾‹å¦‚ï¼ŒåŒ»å­¦ç™¾ç§‘å…¨ä¹¦å’ŒåŒ»å­¦æ–‡ç« )ä¸­æå–åŒ»å­¦QAå¯¹ã€‚æˆ‘ä»¬åœ¨ä¸­æ–‡ç»´åŸºç™¾ç§‘ä¸Šæ”¶é›†äº†8699ä¸ªç–¾ç—…ç™¾ç§‘æ¡ç›®å’Œ2736ä¸ªè¯ç‰©ç™¾ç§‘æ¡ç›®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»é’±æ–‡å¥åº·ç½‘ç«™æŠ“å–äº†226432ç¯‡é«˜è´¨é‡çš„åŒ»å­¦æ–‡ç« ã€‚| [ä¸‹è½½é“¾æ¥](https://huggingface.co/datasets/FreedomIntelligence/huatuo_encyclopedia_qa) | 605MB |
| ä¸­æ–‡åŒ»ç–—å¯¹è¯æ•°æ®é›†(åä½—é¡¹ç›®) | 22ä¸‡æ¡ä¸­æ–‡åŒ»ç–—å¯¹è¯æ•°æ®é›†(åä½—é¡¹ç›®)ï¼šFreedomIntelligence/HuatuoGPT-sft-data-v1 | [ä¸‹è½½é“¾æ¥](https://huggingface.co/datasets/FreedomIntelligence/HuatuoGPT-sft-data-v1) | 333MB |
|åŒ»ç–—å¤§æ¨¡å‹æ•°æ®é›†(åŒ…æ‹¬é¢„è®­ç»ƒã€æŒ‡ä»¤å¾®è°ƒå’Œå¥–åŠ±æ•°æ®é›†) | 240ä¸‡æ¡ä¸­æ–‡åŒ»ç–—æ•°æ®é›†(åŒ…æ‹¬é¢„è®­ç»ƒã€æŒ‡ä»¤å¾®è°ƒå’Œå¥–åŠ±æ•°æ®é›†) | [ä¸‹è½½é“¾æ¥](https://huggingface.co/datasets/shibing624/medical) | 2.1GB |
|å¤–ç§‘é—®è¯Šæ•°æ®BillGPT/Chinese-medical-dialogue-data | 60.8Kæ¡å¤–ç§‘é—®è¯Šæ•°æ®ï¼Œç¤ºä¾‹ï¼š"æ‚£è€…:æ–°ç™€ç‰‡æœ‰ä»€ä¹ˆç”¨,æƒ³é—®ä¸€ä¸‹æ–°ç™€ç‰‡åƒäº†æœ‰ä»€ä¹ˆä½œç”¨å‘€ï¼Ÿ åŒ»ç”Ÿ:ç—…æƒ…åˆ†æï¼šæ‚¨å¥½ï¼šæ–°ç™€ç‰‡ä¸»è¦æ˜¯å¯ä»¥æ¸…çƒ­è§£æ¯’ï¼Œæ´»è¡€åŒ–ç˜€ï¼Œæ¶ˆè‚¿æ­¢ç—›ã€‚ç”¨äºçƒ­æ¯’ç˜€è¡€æ‰€è‡´çš„å’½å–‰è‚¿ç—›ã€ç‰™ç—›ã€ç—¹ç—›ã€èƒç—›ã€é»„ç–¸ã€æ— åè‚¿æ¯’ç­‰ç—‡ã€‚æŒ‡å¯¼æ„è§ï¼šå¦‚æœæ‚¨æœ‰å’½å–‰ç–¼ç—›ç­‰ç—‡çŠ¶æœç”¨æ•ˆæœæ˜¯å¾ˆå¥½çš„ï¼Œä½†æ˜¯æœ‰èƒƒç‚çš„æœ‹å‹å°½é‡ä¸è¦æœç”¨ï¼Œæœ‰ä¸€å®šçš„èƒƒè‚ ååº”ï¼Œé‡Œé¢ä¹Ÿå«æœ‰å¯¹èƒƒæœ‰åˆºæ¿€æˆåˆ†ã€‚" | [ä¸‹è½½é“¾æ¥](https://huggingface.co/datasets/BillGPT/Chinese-medical-dialogue-data) | 936MB |
|ä¸­æ–‡åŒ»å­¦æŒ‡ä»¤ç²¾è°ƒ/æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†(Instruct-tuning)| é‡‡ç”¨äº†å…¬å¼€å’Œè‡ªå»ºçš„ä¸­æ–‡åŒ»å­¦çŸ¥è¯†åº“ï¼Œä¸»è¦å‚è€ƒäº†cMeKGã€‚ åŒ»å­¦çŸ¥è¯†åº“å›´ç»•ç–¾ç—…ã€è¯ç‰©ã€æ£€æŸ¥æŒ‡æ ‡ç­‰æ„å»ºï¼Œå­—æ®µåŒ…æ‹¬å¹¶å‘ç—‡ï¼Œé«˜å±å› ç´ ï¼Œç»„ç»‡å­¦æ£€æŸ¥ï¼Œä¸´åºŠç—‡çŠ¶ï¼Œè¯ç‰©æ²»ç–—ï¼Œè¾…åŠ©æ²»ç–—ç­‰ã€‚åˆ©ç”¨GPT3.5æ¥å£å›´ç»•åŒ»å­¦çŸ¥è¯†åº“æ„å»ºé—®ç­”æ•°æ®ï¼Œè®¾ç½®äº†å¤šç§Promptå½¢å¼æ¥å……åˆ†åˆ©ç”¨çŸ¥è¯†ã€‚ | [ä¸‹è½½é“¾æ¥](https://github.com/SCIR-HI/Med-ChatGLM) | 7.6Kæ¡ |
|MeChatï¼šä¸­æ–‡å¿ƒç†å¥åº·æ”¯æŒå¯¹è¯å¤§æ¨¡å‹ä¸æ•°æ®é›† | æ•°æ®é›†é€šè¿‡ ChatGPT æ”¹å†™çœŸå®çš„å¿ƒç†äº’åŠ© QA ä¸ºå¤šè½®çš„å¿ƒç†å¥åº·æ”¯æŒå¤šè½®å¯¹è¯ï¼ˆsingle-turn to multi-turn inclusive language expansion via ChatGPTï¼‰ï¼Œè¯¥æ•°æ®é›†å«æœ‰ 56k ä¸ªå¤šè½®å¯¹è¯ï¼Œå…¶å¯¹è¯ä¸»é¢˜ã€è¯æ±‡å’Œç¯‡ç« è¯­ä¹‰æ›´åŠ ä¸°å¯Œå¤šæ ·ï¼Œæ›´åŠ ç¬¦åˆåœ¨é•¿ç¨‹å¤šè½®å¯¹è¯çš„åº”ç”¨åœºæ™¯ã€‚ | [ä¸‹è½½é“¾æ¥](https://github.com/qiuhuachuan/smile) | 56kæ¡ |
| CMB-Chinese Medical Benchmark |CMBæ˜¯ä¸€ä¸ªå…¨æ–¹ä½å¤šå±‚æ¬¡çš„ä¸­æ–‡åŒ»ç–—æ¨¡å‹è¯„ä¼°å¹³å°ã€‚å®ƒå…±åŒ…å«280839é“å¤šé¡¹é€‰æ‹©é¢˜å’Œ74ä¾‹å¤æ‚ç—…ä¾‹é—®è¯Šé¢˜ï¼Œæ¶µç›–äº†æ‰€æœ‰åŒ»å­¦ä¸´åºŠå·¥ç§å’Œä¸åŒèŒä¸šçº§åˆ«çš„è€ƒè¯•ï¼Œç»¼åˆè€ƒå¯Ÿæ¨¡å‹çš„åŒ»å­¦çŸ¥è¯†å’Œä¸´åºŠé—®è¯Šèƒ½åŠ› | [ä¸‹è½½é“¾æ¥](https://github.com/FreedomIntelligence/CMB) | 30MB|
|ChatMed_Consult_Dataset  | ChatMed_Consult_Dataset ä¸­çš„query(æˆ–è€…æ˜¯prompt)æ¥è‡ªäºäº’è”ç½‘ä¸Šçš„åŒ»ç–—é—®è¯Šé—®é¢˜(110,113)ï¼Œåæ˜ äº†çœŸå®ä¸–ç•Œçš„ä¸åŒç”¨æˆ·/æ‚£è€…çš„åŒ»ç–—é—®è¯Šéœ€æ±‚ã€‚ç›®å‰responseéƒ½æ˜¯ç”±OpenAI GPT-3.5å¼•æ“å›ç­”çš„ã€‚æˆ‘ä»¬åç»­ä¼šå¯¹äº’è”ç½‘ä¸Šçš„åŒ»ç”Ÿå›ç­”ä¸æ‚£è€…å›ç­”è¿›è¡Œç­›é€‰ç”„åˆ«ï¼Œæ‹©ä¼˜é€‰æ‹©ï¼Œæ„å»ºè´¨é‡æ›´ä¼˜çš„æ•°æ®é›†ã€‚|  [ä¸‹è½½é“¾æ¥](https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset)   | 395MB |
| ä¸­åŒ»è¯æŒ‡ä»¤æ•°æ®é›†ChatMed_TCM_Dataset | ä»¥å¼€æºçš„[ä¸­åŒ»è¯çŸ¥è¯†å›¾è°±] (https://github.com/ywjawmw/TCM_KG) ä¸ºåŸºç¡€ï¼Œé‡‡ç”¨ä»¥å®ä½“ä¸ºä¸­å¿ƒçš„è‡ªæŒ‡ä»¤æ–¹æ³•(entity-centric self-instruct)ï¼Œè°ƒç”¨ChatGPTå¾—åˆ°11w+çš„å›´ç»•ä¸­åŒ»è¯çš„æŒ‡ä»¤æ•°æ®ã€‚ |  [ä¸‹è½½é“¾æ¥](https://huggingface.co/datasets/michaelwzhu/ShenNong_TCM_Dataset) | 110MB |
| cMedQAä¸­æ–‡ç¤¾åŒºåŒ»å­¦é—®ç­”æ•°æ®é›† |åäººç¤¾åŒºåŒ»ç–—é—®ç­”çš„æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ˜¯1.0ç‰ˆæœ¬ï¼Œæä¾›æ–¹å°†ä¸æ—¶æ›´æ–°å’Œæ‰©å……æ•°æ®åº“ã€‚ä¸ºäº†ä¿æŠ¤éšç§ï¼Œæ•°æ®æ˜¯åŒ¿åçš„ï¼Œä¸åŒ…æ‹¬ä¸ªäººä¿¡æ¯ã€‚| [ä¸‹è½½é“¾æ¥](https://github.com/zhangsheng93/cMedQA2) | 80MB |
| WebMedQA çº¿ä¸ŠåŒ»å­¦QA | WebMedQAæ˜¯ä¸€ä¸ªä»ç™¾åº¦åŒ»ç”Ÿå’Œ120Askç­‰åœ¨çº¿å¥åº·å’¨è¯¢ç½‘ç«™æ”¶é›†çš„çœŸå®ä¸­å›½åŒ»å­¦é—®ç­”æ•°æ®é›†ã€‚ç”¨æˆ·é¦–å…ˆå¡«å†™ä¸ªäººä¿¡æ¯è¡¨æ ¼ï¼Œç„¶åæè¿°ä»–ä»¬çš„ç–¾ç—…å’Œå¥åº·é—®é¢˜ã€‚è¿™äº›é—®é¢˜å¯¹æ‰€æœ‰æ³¨å†Œçš„ä¸´åºŠåŒ»ç”Ÿå’Œç”¨æˆ·å¼€æ”¾ï¼Œç›´åˆ°é—®é¢˜æå‡ºè€…é€‰æ‹©æœ€æ»¡æ„çš„ç­”æ¡ˆå¹¶ç»“æŸé—®é¢˜ã€‚åŒ»ç”Ÿå’Œçƒ­å¿ƒçš„ç”¨æˆ·å¯ä»¥åœ¨å‘å¸ƒçš„é—®é¢˜ä¸‹æä¾›è¯Šæ–­å’Œå»ºè®®ï¼Œä»–ä»¬çš„æ ‡é¢˜å’Œä¸“ä¸šä¸ä»–ä»¬çš„ç­”æ¡ˆä¸€èµ·æ˜¾ç¤ºã€‚æé—®è€…ä¹Ÿå¯ä»¥è¿›ä¸€æ­¥è¯¢é—®ä»–ä»¬æ˜¯å¦å¯¹å…¶ä¸­ä¸€ä¸ªç­”æ¡ˆæ„Ÿå…´è¶£ã€‚æ¯ä¸ªé—®é¢˜æ‰€å±çš„ç±»åˆ«ä¹Ÿç”±å…¶æå‡ºè€…é€‰æ‹©ã€‚ | [ä¸‹è½½é“¾æ¥](https://tianchi.aliyun.com/dataset/90202) | 75MB |
| ChineseBLUEåŸºå‡† | ChineseBLUEåŸºå‡†ç”±ä¸åŒçš„å¸¦æœ‰è¯­æ–™åº“çš„ç”Ÿç‰©åŒ»å­¦æ–‡æœ¬æŒ–æ˜ä»»åŠ¡ç»„æˆã€‚è¿™äº›ä»»åŠ¡æ¶µç›–äº†å„ç§æ–‡æœ¬ç±»å‹(ç”Ÿç‰©åŒ»å­¦ç½‘ç»œæ•°æ®å’Œä¸´åºŠç¬”è®°)ã€æ•°æ®é›†å¤§å°å’Œéš¾åº¦ï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œçªå‡ºäº†å¸¸è§çš„ç”Ÿç‰©åŒ»å­¦æ–‡æœ¬æŒ–æ˜æŒ‘æˆ˜ã€‚ | [ä¸‹è½½é“¾æ¥](https://github.com/CBLUEbenchmark/CBLUE) | 400MB |
| Yidu-S4K  | å‘½åå®ä½“è¯†åˆ«,å®ä½“åŠå±æ€§æŠ½å–   | [ä¸‹è½½é“¾æ¥](http://openkg.cn/dataset/yidu-s4k)     | 4Kæ¡ |
| Yidu-N7K  | ä¸´åºŠè¯­æ ‡å‡†åŒ–    | [ä¸‹è½½é“¾æ¥](http://openkg.cn/dataset/yidu-n7k ) | 7Kæ¡ |
| HealthCareMagic-100k | æ¥è‡ªHealthCareMagic.comçš„10ä¸‡æ¬¡ç—…äººå’ŒåŒ»ç”Ÿä¹‹é—´çš„çœŸå®å¯¹è¯ | [ä¸‹è½½é“¾æ¥](https://drive.google.com/file/d/1lyfqIwlLSClhgrCutWuEe_IACNq6XNUt/view?usp=sharing) |137MB|
| icliniq-10k | æ¥è‡ªicliniq.comç½‘ç«™çš„ç—…äººå’ŒåŒ»ç”Ÿä¹‹é—´çš„10Kæ¡çœŸå®å¯¹è¯ | [ä¸‹è½½é“¾æ¥](https://drive.google.com/file/d/1ZKbqgYqWc7DJHs3N9TQYQVPdDQmZaClA/view) | 20MB |
| GenMedGPT-5k |5kä»ChatGPT GenMedGPT-5kå’Œç–¾ç—…æ•°æ®åº“ä¸­ç”Ÿæˆäº†æ‚£è€…å’ŒåŒ»ç”Ÿä¹‹é—´çš„å¯¹è¯ã€‚| [ä¸‹è½½é“¾æ¥](https://drive.google.com/file/d/1nDTKZ3wZbZWTkFMBkxlamrzbNz0frugg/view?usp=sharing) | 5Kæ¡ |

### è‹±æ–‡

| **æ•°æ®é›†åç§°**       | **å†…å®¹æ¦‚è¿°**          | **è·å–é“¾æ¥**         | **æ•°æ®å¤§å°**        |
|--------------------|---------------------|---------------------|--------------------|
| MIMIC-III                     | EHR                              | https://mimic.mit.edu/docs/iii/                                                          | 58,976 hospital admissions for 38,597 patients                    |
| MIMIC-IV                      | EHR                              | https://mimic.mit.edu/docs/iv/                                                           | covering a decade of admissions between 2008 and 2019             |
| CPRD                          | EHR                              | https://cprd.com/data                                                                    | over 2,000 primary care practices and include 60 million patients |
| PubMed                        | Scientific Literature            | https://ftp.ncbi.nlm.nih.gov/pubmed/baseline/                                            | 35M citations and abstracts of biomedical literature              |
| PMC                           | Scientific Literature            | https://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_bulk                                             | 8 million full-text article records                               |
| RCT                           | Scientific Literature            | https://github.com/bwallace/RCT-summarization-data                                       | 4,528 abstract                                                    |
| MS$\hat{~}$2                  | Scientific Literature            | https://github.com/allenai/ms2/                                                          | 470,402 abstract                                                  |
| CDSR                          | Scientific Literature            | https://github.com/qiuweipku/Plain\_language\_summarization                              | 7,805 abstract                                                    |
| SumPubMed                     | Scientific Literature            | https://github.com/vgupta123/sumpubmed                                                   | 33,772 abstract                                                   |
| The Pile                      | Scientific Literature            | https://pile.eleuther.ai/                                                                | 825 GB English text                                               |
| S2ORC                         | Scientific Literature            | https://github.com/jbshp/GenCompareSum                                                   | 63,709 abstract                                                   |
| CORD-19                       | Scientific Literature            | https://github.com/allenai/cord19                                                        | 1M papers                                                         |
| MeQSum                        | Medical Question Summarization   | https://github.com/abachaa/MeQSum                                                        | 1000 instances                                                    |
| CHQ-Sum                       | Medical Question Summarization   | https://github.com/shwetanlp/Yahoo-CHQ-Summ                                              | 1507 instances                                                    |
| UMLS                          | Knowledge Base                   | https://www.nlm.nih.gov/research/umls/index.html                                         | 2M entities for 900K concepts                                     |
| COMETA                        | Web Data (social media)          | https://github.com/cambridgeltl/cometa                                                   | 800K Reddit posts                                                 |
| MedDialog                     | Dialogue                         | https://github.com/UCSD-AI4H/COVID-Dialogue                                              | 3.66 million conversations                                        |
| CovidDialog                   | Dialogue                         | https://github.com/UCSD-AI4H/COVID-Dialogue                                              | 603 consultations                                                 |
| Medical Flashcards            | Dialogue                         | https://github.com/kbressem/medalpaca                                                    | 33955 instances                                                   |
| Wikidoc                       | Dialogue                         | https://huggingface.co/datasets/medalpaca/medical\_meadow\_wikidoc                       | 67704 instances                                                   |
| Wikidoc Patient Information   | Dialogue                         | https://huggingface.co/datasets/medalpaca/medical\_meadow\_wikidoc\_patient\_information | 5942 instances                                                    |
| MEDIQA                        | Dialogue                         | https://huggingface.co/datasets/medalpaca/medical\_meadow\_wikidoc\_patient\_information | 2208 instances                                                    |
| CORD-19                       | Dialogue                         | https://huggingface.co/datasets/medalpaca/medical\_meadow\_cord19                        | 1056660 instances                                                 |
| MMMLU                         | Dialogue                         | https://huggingface.co/datasets/medalpaca/medical\_meadow\_mmmlu                         | 3787 instances                                                    |
| Pubmed Causal                 | Dialogue                         | https://huggingface.co/datasets/medalpaca/medical\_meadow\_pubmed\_causal                | 2446 instances                                                    |
| ChatDoctor                    | Dialogue                         | https://github.com/Kent0n-Li/ChatDoctor                                                  | 215000 instances                                                  |
| Alpaca-EN-AN                  | English Instructions             | https://github.com/tatsu-lab/stanford\_alpaca/blob/main/alpaca\_data.json                | 52K instructions                                                  |
| Alpaca-CH-AN                  | Chinese Instructions             | https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/tree/main/data                | 52K instructions                                                  |
| ShareGPT                      | Conversations                    | https://huggingface.co/datasets/philschmid/sharegpt-raw                                  | 61653 long conversations                                          |
| WebText                       | Web Data                         | https://commoncrawl.org/the-data/get-started/                                            | 40 GB of text                                                     |
| OpenWebText                   | Web Data                         | https://skylion007.github.io/OpenWebTextCorpus/                                          | 38 GB of text                                                     |
| Colossal Clean Crawled Corpus | Web Data                         | https://www.tensorflow.org/datasets/catalog/c4                                           | 806 GB of text                                                    |
| OpenI                         | EHR, Multimodel                  | https://openi.nlm.nih.gov/faq\#collection                                                | 3.7 million images from about 1.2 million papers                  |
| U-Xray                        | Multimodel                       | https://openi.nlm.nih.gov/                                                               | 3,955 reports and 7,470 images                                    |
| ROCO                          | Multimodel                       | https://github.com/razorx89/roco-dataset                                                 | 81,000 radiology images and corresponding captions                |
| MedICaT                       | Multimodel                       | https://github.com/allenai/medicat                                                       | 17,000 images includes captions                                   |
| PMC-OA                        | Multimodel                       | https://huggingface.co/datasets/axiong/pmc\_oa\_beta                                     | 1.6M image-caption pairs                                          |
| CheXpert                      | Multimodel                       | https://aimi.stanford.edu/chexpert-chest-x-rays                                          | 224,316 chest radiographs with associated reports                 |
| PadChest                      | Multimodel                       | http://bimcv.cipf.es/bimcv-projects/padchest/                                            | 160,000 images  with related text                                 |
| MIMIC-CXR                     | Multimodel                       | https://mimic.mit.edu/docs/iv/modules/cxr/                                               | 227,835 imaging studies for 64,588 patients                       |
| PMC-15M                       | Multimodel                       |                                                                                          | 15 million Figure-caption                                         |
| pairs                         | https://arxiv.org/abs/2303.00915 |                                                                                          |                                                                   |
| OpenPath                      | Multimodel                       | https://laion.ai/blog/laion-5b/                                                          | 208,414 pathology images related descriptions                     |


## Models
| Title | Institute | Date | Code
| :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------: | :-----------: | :---------: |
|[DoctorGPT](https://huggingface.co/llSourcell/medllama2_7b) | Sirajraval, GPT School | 2023-08 | [Github](https://github.com/llSourcell/DoctorGPT) 
| [CoDoC: Enhancing the reliability and accuracy of AI-enabled diagnosis via complementarity-driven deferral to clinicians](https://www.nature.com/articles/s41591-023-02437-x) | DeepMind, Google <br> ![citations](https://img.shields.io/badge/dynamic/json?url=https://api.semanticscholar.org/graph/v1/paper/CorpusID:259939727?fields=citationCount&query=%24.citationCount&label=citations) | 2023-07 | [Github](https://github.com/deepmind/codoc/tree/main) <br> ![Star](https://img.shields.io/github/stars/deepmind/codoc?style=social&label=Stars)
| [Med-PaLM 2: Towards Expert-Level Medical Question Answering with Large Language Models](https://arxiv.org/pdf/2305.09617.pdf) | Google <br> ![citations](https://img.shields.io/badge/dynamic/json?url=https://api.semanticscholar.org/graph/v1/paper/CorpusID:258715226?fields=citationCount&query=%24.citationCount&label=citations) | 2023-05 | -
| [Capabilities of GPT-4 on Medical Challenge Problems](https://arxiv.org/pdf/2303.13375.pdf) | Microsoft, OpenAI <br> ![citations](https://img.shields.io/badge/dynamic/json?url=https://api.semanticscholar.org/graph/v1/paper/CorpusID:257687695?fields=citationCount&query=%24.citationCount&label=citations) | 2023-03 | -
| [BioMedLM-PubMedGPT: A purpose-built AI model trained to interpret biomedical language](https://crfm.stanford.edu/2022/12/15/biomedlm.html) | Stanford CRFM, MosaicML | 2022-12 | [HuggingFace](https://huggingface.co/stanford-crfm/BioMedLM) <br> ![Likes](https://img.shields.io/badge/dynamic/json?url=https://huggingface.co/api/models/stanford-crfm/BioMedLM&query=%24.likes&label=ğŸ¤—+Likes)
| [Med-PaLM: Large Language Models Encode Clinical Knowledge](https://arxiv.org/pdf/2212.13138.pdf) | Google <br> ![citations](https://img.shields.io/badge/dynamic/json?url=https://api.semanticscholar.org/graph/v1/paper/CorpusID:255124952?fields=citationCount&query=%24.citationCount&label=citations) | 2022-12 | [Github](https://github.com/conceptofmind/PaLM) <br> ![Star](https://img.shields.io/github/stars/conceptofmind/PaLM?style=social&label=Stars)
| [ClinicalT5: A Generative Language Model for Clinical Text](https://aclanthology.org/2022.findings-emnlp.398.pdf) | University of Oregon, Baidu <br> ![citations](https://img.shields.io/badge/dynamic/json?url=https://api.semanticscholar.org/graph/v1/paper/CorpusID:256631112?fields=citationCount&query=%24.citationCount&label=citations) | 2022-12 | [HuggingFace](https://huggingface.co/luqh/ClinicalT5-large) <br> ![Likes](https://img.shields.io/badge/dynamic/json?url=https://huggingface.co/api/models/luqh/ClinicalT5-large&query=%24.likes&label=ğŸ¤—+Likes)
| [GatorTron: A large language model for electronic health records](https://www.nature.com/articles/s41746-022-00742-2) | University of Florida, NVIDIA <br> ![citations](https://img.shields.io/badge/dynamic/json?url=https://api.semanticscholar.org/graph/v1/paper/CorpusID:255175535?fields=citationCount&query=%24.citationCount&label=citations) | 2022-12 | [HuggingFace](https://huggingface.co/UFNLP/gatortron-base) <br> ![Likes](https://img.shields.io/badge/dynamic/json?url=https://huggingface.co/api/models/UFNLP/gatortron-base&query=%24.likes&label=ğŸ¤—+Likes)
| [BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining](https://aclanthology.org/2020.clinicalnlp-1.17/) | Microsoft Research <br> ![citations](https://img.shields.io/badge/dynamic/json?url=https://api.semanticscholar.org/graph/v1/paper/CorpusID:252542956?fields=citationCount&query=%24.citationCount&label=citations) | 2022-09 | [HuggingFace](https://huggingface.co/microsoft/biogpt) <br> ![Likes](https://img.shields.io/badge/dynamic/json?url=https://huggingface.co/api/models/microsoft/biogpt&query=%24.likes&label=ğŸ¤—+Likes)
| [BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model](https://arxiv.org/pdf/2204.03905.pdf) | Tsinghua University <br> ![citations](https://img.shields.io/badge/dynamic/json?url=https://api.semanticscholar.org/graph/v1/paper/CorpusID:248069469?fields=citationCount&query=%24.citationCount&label=citations) | 2022-04 | [HuggingFace](https://huggingface.co/GanjinZero/biobart-v2-base) <br> ![Likes](https://img.shields.io/badge/dynamic/json?url=https://huggingface.co/api/models/GanjinZero/biobart-v2-base&query=%24.likes&label=ğŸ¤—+Likes)
| [KeBioLM: Improving Biomedical Pretrained Language Models with Knowledge](https://aclanthology.org/2021.bionlp-1.20.pdf) | Tsinghua, Alibaba <br> ![citations](https://img.shields.io/badge/dynamic/json?url=https://api.semanticscholar.org/graph/v1/paper/CorpusID:233324564?fields=citationCount&query=%24.citationCount&label=citations) | 2021-04 | [Github](https://github.com/GanjinZero/KeBioLM) <br> ![Star](https://img.shields.io/github/stars/GanjinZero/KeBioLM?style=social&label=Stars)
| [Pretrained Language Models for Biomedical and Clinical Tasks: Understanding and Extending the State-of-the-Art](https://aclanthology.org/2020.clinicalnlp-1.17/) | Meta <br> ![citations](https://img.shields.io/badge/dynamic/json?url=https://api.semanticscholar.org/graph/v1/paper/CorpusID:226283910?fields=citationCount&query=%24.citationCount&label=citations) | 2020-11 | [Github](https://github.com/facebookresearch/bio-lm) <br> ![Star](https://img.shields.io/github/stars/facebookresearch/bio-lm?style=social&label=Stars)
| [BioMegatron: Larger Biomedical Domain Language Model](https://aclanthology.org/2020.emnlp-main.379.pdf) | NVIDIA <br> ![citations](https://img.shields.io/badge/dynamic/json?url=https://api.semanticscholar.org/graph/v1/paper/CorpusID:222310618?fields=citationCount&query=%24.citationCount&label=citations) | 2020-10 | [HuggingFace](https://huggingface.co/EMBO/BioMegatron345mUncased) <br> ![Likes](https://img.shields.io/badge/dynamic/json?url=https://huggingface.co/api/models/EMBO/BioMegatron345mUncased&query=%24.likes&label=ğŸ¤—+Likes)
| [PubMedBERT: Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing](https://arxiv.org/pdf/2007.15779.pdf) | Microsoft Research <br> ![citations](https://img.shields.io/badge/dynamic/json?url=https://api.semanticscholar.org/graph/v1/paper/CorpusID:220919723?fields=citationCount&query=%24.citationCount&label=citations) | 2020-07 | [HuggingFace](https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext) <br> ![Likes](https://img.shields.io/badge/dynamic/json?url=https://huggingface.co/api/models/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext&query=%24.likes&label=ğŸ¤—+Likes)
| [Publicly Available Clinical BERT Embeddings](https://arxiv.org/pdf/1904.03323.pdf) | MIT CSAIL <br> ![citations](https://img.shields.io/badge/dynamic/json?url=https://api.semanticscholar.org/graph/v1/paper/CorpusID:102352093?fields=citationCount&query=%24.citationCount&label=citations) | 2019-04 | [HuggingFace](https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT) <br> ![Likes](https://img.shields.io/badge/dynamic/json?url=https://huggingface.co/api/models/emilyalsentzer/Bio_ClinicalBERT&query=%24.likes&label=ğŸ¤—+Likes)
| [ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission](https://arxiv.org/pdf/1904.05342.pdf) | Harvard, Princeton, NYU <br> ![citations](https://img.shields.io/badge/dynamic/json?url=https://api.semanticscholar.org/graph/v1/paper/CorpusID:119308351?fields=citationCount&query=%24.citationCount&label=citations) | 2019-04 | [Github](https://github.com/kexinhuang12345/clinicalBERT) <br> ![Star](https://img.shields.io/github/stars/kexinhuang12345/clinicalBERT?style=social&label=Stars)
| [BioBERT: a pre-trained biomedical language representation model for biomedical text mining](https://arxiv.org/pdf/1901.08746) | Korea University <br> ![citations](https://img.shields.io/badge/dynamic/json?url=https://api.semanticscholar.org/graph/v1/paper/CorpusID:59291975?fields=citationCount&query=%24.citationCount&label=citations) | 2019-01 | [Github](https://github.com/dmis-lab/biobert) <br> ![Star](https://img.shields.io/github/stars/dmis-lab/biobert?style=social&label=Stars)
<!--
| [Paper Name](arxiv link) | Institute Name <br> ![citations](https://img.shields.io/badge/dynamic/json?url=https://api.semanticscholar.org/graph/v1/paper/CorpusID:12345678?fields=citationCount&query=%24.citationCount&label=citations) | 20xx-xx | [HuggingFace](https://github.com) <br> ![Likes](https://img.shields.io/badge/dynamic/json?url=https://huggingface.co/api/models/&query=%24.likes&label=ğŸ¤—+Likes)
-->

## Popular LLMs

| **Model Name**                             | **Base**       | **Para. (B)**     | **Features**                                    | **Date** | **Link**                                                               |
|--------------------------------------------|----------------|-------------------|-------------------------------------------------|----------|------------------------------------------------------------------------|
| GatorTron          | Transformer    | 0.345, 3.9, 8.9   | Training from scratch                           | 06/2022  |  https://github.com/uf-hobi-informatics-lab/GatorTron     |
| Codex-Med              | GPT-3.5        | 175               | CoT, Zero-shot                                  | 07/2022  |  https://github.com/vlievin/medical-reasoning             |
| Galactica        | Transformer    | 1.3, 6.4, 30, 120 | Reasoning, Multidisciplinary                    | 11/2022  |  https://galactica.org                                       |
| Med-PaLM            | Flan-PaLM/PaLM | 540               | CoT, Self-consistency                           | 12/2022  | -                                                                      |
| GPT-4-Med                   | GPT-4          | -                 | no specialized prompt crafting                  | 03/2023  | -                                                                      |
| DeID-GPT                 | GPT-4          | -                 | De-identifying                                  | 03/2023  |  https://github.com/yhydhx/ChatGPT-API                    |
| ChatDoctor    | LLaMA          | 7                 | Retrieve online, external knowledge             | 03/2023  |  https://github.com/Kent0n-Li/ChatDoctor                  |
| DoctorGLM         | ChatGLM        | 6                 | Extra prompt designer                           | 04/2023  |  https://github.com/xionghonglin/DoctorGLM                |
| MedAlpaca           | LLaMA          | 7, 13             | Adapt to Medicine                               | 04/2023  |  https://github.com/kbressem/medAlpaca                    |
| BenTsao               | LLaMA          | 7                 | Knowledge graph                                 | 04/2023  |  https://github.com/SCIR-HI/ Huatuo-Llama-Med-Chinese     |
| PMC-LLaMA                  | LLaMA          | 7                 | Adapt to Medicine                               | 04/2023  |  https://github.com/chaoyi-wu/PMC-LLaMA                   |
| Visual Med-Alpaca  | LLaMA          | 7                 | multimodal generative model, Self-Instruct      | 04/2023  |  https://github.com/cambridgeltl/visual-med-alpaca        |
| BianQue~            | ChatGLM        | 6                 | Chain of Questioning                            | 04/2023  |  https://github.com/scutcyr/BianQue                       |
| Med-PaLM 2        | PaLM 2         | 340               | Ensemble refinement, CoT, Self-consistency      | 05/2023  | -                                                                      |
| GatorTronGPT           | GPT-3          | 5, 20             | Training from scratch for medicine              | 05/2023  |  https://github.com/uf-hobi-informatics-lab/GatorTronGPT  |
| HuatuoGPT         | Bloomz         | 7                 | Reinforced learning from AI feedback            | 05/2023  |  https://github.com/FreedomIntelligence/HuatuoGPT         |
| ClinicalGPT      | BLOOM          | 7                 | multi-round dialogue consultations              | 06/2023  | -                                                                      |
| MedAGI                  | MiniGPT-4      | -                 | multimodal, AGI                                 | 06/2023  |  https://github.com/JoshuaChou2018/MedAGI                 |
| LLaVA-Med                | LLaVA          | 13                | multimodal, self-instruct,  curriculum learning | 06/2023  |  https://github.com/microsoft/LLaVA-Med                   |
| OphGLM                 | ChatGLM        | 6                 | multimodal, Ophthalmology LLM                   | 06/2023  |  https://github.com/ML-AILab/OphGLM                       |
| SoulChat            | ChatGLM        | 6                 | Mental Healthcare                               | 06/2023  |  https://github.com/scutcyr/SoulChat                      |
| Med-Flamingo             | Flamingo       | 80               | multimodal, Few-Shot generative medical VQA     | 07/2023  |  https://github.com/snap-stanford/med-flamingo            |



## Papers

- Med-PaLM 2: Towards Expert-Level Medical Question Answering with Large Language Models [[Paper]](https://arxiv.org/pdf/2305.09617.pdf)
- KeBioLM: Improving Biomedical Pretrained Language Models with Knowledge [[Paper]](https://arxiv.org/abs/2104.10344)
- BioELMo: Probing Biomedical Embeddings from Language Models [[Paper]](https://arxiv.org/abs/1904.02181)
- BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model [[Paper]](https://aclanthology.org/2022.bionlp-1.9.pdf)
- ClinicalT5: A Generative Language Model for Clinical Text [[Paper]](https://aclanthology.org/2022.findings-emnlp.398.pdf)
- GatorTron: A Large Clinical Language Model to Unlock Patient Information from Unstructured Electronic Health Records [[Paper]](https://arxiv.org/pdf/2203.03540v2.pdf)
- ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models [[Paper]](https://arxiv.org/pdf/2302.07257.pdf) [[Code]](https://github.com/zhaozh10/ChatCAD)
- DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4 [[Paper]](https://arxiv.org/pdf/2303.11032.pdf)
- Capabilities of GPT-4 on Medical Challenge Problems [[Paper]](https://arxiv.org/pdf/2303.13375.pdf)
- BioBERT: a pre-trained biomedical language representation model for biomedical text mining [[Paper]](https://arxiv.org/pdf/1901.08746.pdf)
- Publicly Available Clinical BERT Embeddings [[Paper]](https://arxiv.org/pdf/1904.03323.pdf)
- BioMegatron: Larger Biomedical Domain Language Model [[Paper]](https://arxiv.org/pdf/2010.06060.pdf)
- Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks [[Paper]](https://aclanthology.org/2020.acl-main.740.pdf)
- Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction [[Paper]](https://www.nature.com/articles/s41746-021-00455-y)
- DoctorGLM: Fine-tuning your chinese doctor is not a herculean task [[Paper]](https://arxiv.org/abs/2304.01097) [[Code]](https://github.com/xionghonglin/DoctorGLM)
- HuatuoGPT, Towards Taming Language Models To Be a Doctor [[Paper]](https://arxiv.org/abs/2305.15075) [[Code]](https://github.com/FreedomIntelligence/HuatuoGPT)
- BioELECTRA:Pretrained Biomedical text Encoder using Discriminators [[Paper]](https://aclanthology.org/2021.bionlp-1.16.pdf)
- LinkBERT: Pretraining Language Models with Document Links [[Paper]](https://arxiv.org/pdf/2203.15827.pdf)
- BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining [[Paper]](https://arxiv.org/pdf/2210.10341.pdf)
- Large Language Models Encode Clinical Knowledge [[Paper]](https://arxiv.org/pdf/2212.13138.pdf)
- A large language model for electronic health records [[Paper]](https://www.nature.com/articles/s41746-022-00742-2)
- Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing [[Paper]](https://arxiv.org/pdf/2007.15779.pdf)
- BEHRT: Transformer for Electronic Health Records [[Paper]](https://www.nature.com/articles/s41598-020-62922-y)
- Federated Learning of Medical Concepts Embedding using BEHRT [[Paper]](https://arxiv.org/abs/2305.13052) [[Code]](https://github.com/nadavlab/FederatedBEHRT)
- RadBERT: Adapting Transformer-based Language Models to Radiology [[paper]](https://pubs.rsna.org/doi/epdf/10.1148/ryai.210258) [[HuggingFace]](https://huggingface.co/UCSD-VA-health/RadBERT-RoBERTa-4m)
- Highly accurate protein structure prediction with AlphaFold [[Paper]](https://www.nature.com/articles/s41586-021-03819-2) [[Code]](https://github.com/deepmind/alphafold)
- Accurate prediction of protein structures and interactions using a three-track neural network [[Paper]](https://www.science.org/doi/full/10.1126/science.abj8754?casa_token=tleEHPOOSr8AAAAA%3AT0eToIMPW0oN1jjIGLs8aPyQK8qbcFIByjT1x4k90tvBAj03SZUzpEinCPe_t-g4ECmjJ9wlj8OwQBs)
- Protein complex prediction with AlphaFold-Multimer [[Paper]](https://www.biorxiv.org/content/10.1101/2021.10.04.463034v2.abstract)
- FastFold: Reducing AlphaFold Training Time from 11 Days to 67 Hours [[Paper]](https://arxiv.org/abs/2203.00854) [[Code]](https://github.com/hpcaitech/fastfold)
- HelixFold: An Efficient Implementation of AlphaFold2 using PaddlePaddle [[Paper]](https://arxiv.org/abs/2207.05477) [[Code]](https://github.com/PaddlePaddle/PaddleHelix)
- Uni-Fold: An Open-Source Platform for Developing Protein Folding Models beyond AlphaFold [[Paper]](https://www.biorxiv.org/content/10.1101/2022.08.04.502811v3.abstract) [[Code]](https://github.com/dptech-corp/Uni-Fold)
- OpenFold: Retraining AlphaFold2 yields new insights into its learning mechanisms and capacity for generalization [[Paper]](https://www.biorxiv.org/content/10.1101/2022.11.20.517210v2.abstract) [[Code]](https://github.com/aqlaboratory/openfold)
- ManyFold: an efficient and flexible library for training and validating protein folding models [[Paper]](https://academic.oup.com/bioinformatics/article/39/1/btac773/6887136) [[Code]](https://github.com/instadeepai/manyfold)
- ColabFold: making protein folding accessible to all [[Paper]](https://www.nature.com/articles/s41592-022-01488-1) [[Code]](https://github.com/sokrypton/ColabFold)
- Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences [[Paper]](https://www.pnas.org/doi/abs/10.1073/pnas.2016239118) [[Code]](https://github.com/facebookresearch/esm)
- ProGen: Language Modeling for Protein Generation [[Paper]](https://arxiv.org/abs/2004.03497) [[Code]](https://github.com/lucidrains/progen)
- ProtTrans: Towards Cracking the Language of Life's Code Through Self-Supervised Deep Learning and High Performance Computing [[Paper]](https://arxiv.org/abs/2007.06225) [[Code]](https://github.com/agemagician/ProtTrans)
- Evolutionary-scale prediction of atomic level protein structure with a language model [[Paper]](https://www.science.org/doi/full/10.1126/science.ade2574)
- High-resolution de novo structure prediction from primary sequence [[Paper]](https://www.biorxiv.org/content/10.1101/2022.07.21.500999v1.abstract) [[Code]](https://github.com/HeliXonProtein/OmegaFold)
- Single-sequence protein structure prediction using a language model and deep learning [[Paper]](https://www.nature.com/articles/s41587-022-01432-w)
- Improved the Protein Complex Prediction with Protein Language Models [[Paper]](https://www.biorxiv.org/content/10.1101/2022.09.15.508065v2.abstract)
- MSA Transformer [[Paper]](http://proceedings.mlr.press/v139/rao21a.html) [[Code]](https://github.com/The-AI-Summer/self-attention-cv)
- Deciphering antibody affinity maturation with language models and weakly supervised learning [[Paper]](https://arxiv.org/abs/2112.07782)
- xTrimoABFold: De novo Antibody Structure Prediction without MSA [[Paper]](https://arxiv.org/abs/2212.00735)
- scBERT as a large-scale pretrained deep language model for cell type annotation of single-cell RNA-seq data [[Paper]](https://arxiv.org/abs/2212.00735) [[Code]](https://github.com/TencentAILabHealthcare/scBERT)
- Interpretable RNA Foundation Model from Unannotated Data for Highly Accurate RNA Structure and Function Predictions [[Paper]](https://www.biorxiv.org/content/10.1101/2022.08.06.503062v2.abstract) [[Code]](https://github.com/ml4bio/rna-fm)
- E2Efold-3D: End-to-End Deep Learning Method for accurate de novo RNA 3D Structure Prediction [[Paper]](https://www.biorxiv.org/content/10.1101/2022.08.06.503062v2.abstract) [[Code]](https://github.com/ml4bio/rna-fm)


## Other Awesome List
[LLM-for-Healthcare](https://github.com/KaiHe-better/LLM-for-Healthcare) - A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics.

## Licenses

[![MIT license](https://img.shields.io/badge/License-MIT-blue.svg)](https://lbesson.mit-license.org/)

æœ¬é¡¹ç›®éµå¾ª [MIT License](https://lbesson.mit-license.org/).

[![CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](http://creativecommons.org/licenses/by-nc-sa/4.0/)

æœ¬é¡¹ç›®éµå¾ª [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/).



## å¼•ç”¨

å¦‚æœæœ¬é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„é¡¹ç›®ã€‚

```
@misc{medllmdata2023,
  author = {Jun Wang, Changyu Hou, Pengyong Li, Jingjing Gong ,Chen Song, Qi Shen, Guotong Xie},
  title = {Awesome Dataset for Medical LLM: A curated list of popular Datasets, Models and Papers for LLMs in Medical/Healthcare},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/onejune2018/Awesome-Medical-Healthcare-Dataset-For-LLM}},
}
â€‹```
